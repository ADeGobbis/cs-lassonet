<html>

<head>
  <link rel="stylesheet" href="style.css">
  <title>LassoNet: Neural Networks with Feature Sparsity</title>
  <!-- <meta property="og:image" content="" /> -->
  <meta property="og:title" content="LassoNet: Neural Networks with Feature Sparsity " />
</head>

<body>
  <br>
  <h1 class=center>LassoNet: Neural Networks with Feature Sparsity</h1>

  <table id=authors class=center>
    <tr>
      <td>
        <a href="https://ismael.lemhadri.org/">Ismael Lemhadri</a>
      </td>
      <td>
        <a href="https://fengruan.github.io/">Feng Ruan</a>
      </td>
      <td>
        <a href="https://louisabraham.github.io/">Louis Abraham</a>
      </td>
      <td>
        <a href="https://statweb.stanford.edu/~tibs/">Rob Tibshirani</a>
      </td>
  </table>

  <table id=links class=center>
    <tr>
      <td>
        <a href='https://arxiv.org/pdf/1907.12207.pdf'>[Paper]</a>
      </td>
      <td>
        <a href='https://github.com/ilemhadri/lassonet'>[GitHub]</a>
      </td>
      <td>
        <a href='./api'>[Docs]</a>
      </td>
      <td>
        <a href='https://ismael.lemhadri.org/papers/pdf/lassonet_poster.pdf'>[Poster]</a>
      </td>
      <td>
        <a href='https://ismael.lemhadri.org/papers/pdf/lassonet_slides.pdf'>[Slides]</a>
      </td>
    </tr>
  </table>

  <img class="center" src="fig1.png" height="300px"></img>

  <h1>Abstract</h1>

  <p>Much work has been done recently to make neural networks more interpretable, and one obvious approach is to arrange
    for the network to use only a subset of the available features. In linear models, Lasso (or L1-regularized)
    regression assigns zero weights to the most irrelevant or redundant features, and is widely used in data science.
    However the Lasso only applies to linear models. Here we introduce LassoNet, a neural network framework with global
    feature selection. Our approach enforces a hierarchy: specifically a feature can participate in a hidden unit only
    if its linear representative is active. Unlike other approaches to feature selection for neural nets, our method
    uses a modified objective function with constraints, and so integrates feature selection with the parameter learning
    directly. As a result, it delivers an entire regularization path of solutions with a range of feature sparsity. On
    systematic experiments, LassoNet significantly outperforms state-of-the-art methods for feature selection and
    regression. The LassoNet method uses projected proximal gradient descent, and generalizes directly to deep networks.
    It can be implemented by adding just a few lines of code to a standard neural network.</p>

  <hr>

  <div>
    <h1>Intro video</h1>
    <iframe src="https://www.youtube.com/embed/G5vPojso9PU" frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowfullscreen></iframe>

    <h1>Talk</h1>
    <iframe src="https://www.youtube.com/embed/ztGcoMPazwc" frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowfullscreen></iframe>
    <br>
  </div>

  <hr>

  <div>
    <h1>Citation</h1>
    Please use the citation below:
    <pre>
      @article{lemhadri2019neural,
        title={LassoNet: Neural Networks with Feature Sparsity},
        author={Lemhadri, Ismael and Ruan, Feng and
                Abraham, Louis and Tibshirani, Robert},
        journal={arXiv preprint arXiv:1907.12207},
        year={2019}
      }
    </pre>
  </div>

</body>

</html>